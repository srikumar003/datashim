{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#datashim","title":"Datashim","text":"<p>Our Framework introduces the Dataset CRD which is a pointer to existing S3 and NFS data sources. It includes the necessary logic to map these Datasets into Persistent Volume Claims and ConfigMaps which users can reference in their pods, letting them focus on the workload development and not on configuring/mounting/tuning the data access. Thanks to Container Storage Interface it is extensible to support additional data sources in the future.</p> <p></p> <p>A Kubernetes Framework to provide easy access to S3 and NFS Datasets within pods. Orchestrates the provisioning of Persistent Volume Claims and ConfigMaps needed for each Dataset. Find more details in our FAQ</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>In order to quickly deploy DLF, based on your environment execute one of the following commands:</p> <ul> <li>Kubernetes/Minikube <pre><code>kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf.yaml\n</code></pre></li> <li>Kubernetes on IBM Cloud <pre><code>kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-ibm-k8s.yaml\n</code></pre></li> <li>Openshift <pre><code>kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-oc.yaml\n</code></pre></li> <li>Openshift on IBM Cloud <pre><code>kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-ibm-oc.yaml\n</code></pre></li> </ul> <p>Wait for all the pods to be ready :) <pre><code>kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=dlf -n dlf\n</code></pre></p> <p>As an optional step, label the namespace you want to have the pods labelling functionality (see below) <pre><code>kubectl label namespace default monitor-pods-datasets=enabled\n</code></pre></p> <p>In case don't have an existing S3 Bucket follow our wiki to deploy an Object Store and populate it with data.</p> <p>We will create now a Dataset named <code>example-dataset</code> pointing to your S3 bucket. <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: com.ie.ibm.hpsys/v1alpha1\nkind: Dataset\nmetadata:\nname: example-dataset\nspec:\nlocal:\ntype: \"COS\"\naccessKeyID: \"{AWS_ACCESS_KEY_ID}\"\nsecretAccessKey: \"{AWS_SECRET_ACCESS_KEY}\"\nendpoint: \"{S3_SERVICE_URL}\"\nbucket: \"{BUCKET_NAME}\"\nreadonly: \"true\" #OPTIONAL, default is false  \nregion: \"\" #OPTIONAL\nEOF\n</code></pre></p> <p>If everything worked okay, you should see a PVC and a ConfigMap named <code>example-dataset</code> which you can mount in your pods. As an easier way to use the Dataset in your pod, you can instead label the pod as follows: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\ndataset.0.id: \"example-dataset\"\ndataset.0.useas: \"mount\"\nspec:\ncontainers:\n- name: nginx\nimage: nginx\n</code></pre></p> <p>As a convention the Dataset will be mounted in <code>/mnt/datasets/example-dataset</code>. If instead you wish to pass the connection details as environment variables, change the <code>useas</code> line to <code>dataset.0.useas: \"configmap\"</code></p> <p>Feel free to explore our examples</p>"},{"location":"Archive-based-Datasets/","title":"Prerequisites","text":"<ul> <li>We will work with the branch master</li> <li>You have kubectl utility installed and your account has admin rights to install service accounts etc</li> <li>For demo purposes you can use minikube.</li> <li>Install Datashim using one of the quickstart environments</li> </ul>"},{"location":"Archive-based-Datasets/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/datashim-io/datashim.git\ncd datashim\n</code></pre> <p>After you check out the project and the correct branch, proceed with the installation of minio.</p> <p>If you already have a cloud object store, you can skip this step. <pre><code>kubectl apply -n dlf -f examples/minio/\n</code></pre> The above will install on the components in the <code>dlf</code> namespace.</p> <p>A final step would be to create a secret named <code>minio-conf</code> in the <code>dlf</code> namespace which would point on the connection information for the cloud object store you would be using. In the case you have provisioned our demo minio instance, execute the below. In different case adopt the connection details to reflect on your setup. <pre><code>kubectl create secret generic minio-conf --from-literal='AWS_ACCESS_KEY_ID=minio' --from-literal='AWS_SECRET_ACCESS_KEY=minio123' --from-literal='ENDPOINT=http://minio-service:9000' -n dlf\n</code></pre></p> <p>You can check the status of the installation: <pre><code>watch kubectl get pods -n dlf\n</code></pre> When all the components are ready the output should look like this: <pre><code>NAME                                READY   STATUS      RESTARTS   AGE\ncsi-attacher-nfsplugin-0            2/2     Running     0          3m1s\ncsi-attacher-s3-0                   1/1     Running     0          3m1s\ncsi-hostpath-attacher-0             1/1     Running     0          3m1s\ncsi-hostpath-provisioner-0          1/1     Running     0          3m1s\ncsi-hostpathplugin-0                3/3     Running     0          3m1s\ncsi-nodeplugin-nfsplugin-vs7d9      2/2     Running     0          3m1s\ncsi-provisioner-s3-0                1/1     Running     0          3m1s\ncsi-s3-mrndx                        2/2     Running     0          3m1s\ndataset-operator-76798546cf-9d6wj   1/1     Running     0          3m1s\ngenerate-keys-n7m5l                 0/1     Completed   0          3m1s\nminio-7979c89d5c-khncd              0/1     Running     0          3m\n</code></pre></p>"},{"location":"Archive-based-Datasets/#usage","title":"Usage","text":"<p>Now we can create a Dataset based on a remote archive as follows: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: com.ie.ibm.hpsys/v1alpha1\nkind: Dataset\nmetadata:\nname: example-dataset\nspec:\ntype: \"ARCHIVE\"\nurl: \"https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\"\nformat: \"application/x-tar\"\nEOF\n</code></pre> You should see now a PVC created with the same name: <pre><code>$ kubectl get pvc\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nexample-dataset   Bound    pvc-c58852a6-a597-4eb8-a05b-23d9899226bf   9314Gi     RWX            csi-s3         15s\n</code></pre> You can reference the dataset in the pod either as a usual PVC or by using the labels as follows: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\ndataset.0.id: \"example-dataset\"\ndataset.0.useas: \"mount\"\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nEOF\n</code></pre> You can exec into the pod (after it has started) and inspect that the Dataset is available as follows: <pre><code>$ kubectl exec -it nginx /bin/bash\nroot@nginx:/# ls /mnt/datasets/example-dataset/\nnoaa-weather-data-jfk-airport\nroot@nginx:/# ls /mnt/datasets/example-dataset/noaa-weather-data-jfk-airport/\nLICENSE.txt  README.txt  clean_data.py  jfk_weather.csv  jfk_weather_cleaned.csv\n</code></pre></p>"},{"location":"Ceph-Caching/","title":"Rook/Ceph Installation","text":""},{"location":"Ceph-Caching/#method-1-recommended","title":"Method 1 (Recommended)","text":"<p>Inside <code>plugins/ceph-cache-plugin/deploy/rook</code> directory execute: <pre><code>kubectl create -f common.yaml\nkubectl create -f operator.yaml\n</code></pre> Inspect the file cluster.yaml and setup according to the nodes and the dedicated ceph-wise disk devices, the value of <code>storage.nodes</code> e.g. <pre><code>storage:\nuseAllNodes: false\nuseAllDevices: false\nnodes:\n- name: \"minikube\"\ndevices: - name: \"sdb\"\nconfig:\nstoreType: bluestore\nosdsPerDevice: \"1\"\n</code></pre> Afterward, execute: <pre><code>kubectl create -f cluster.yaml\n</code></pre> If everything worked correctly the pods in the rook-ceph namespace should be like this: <pre><code>rook-ceph-mgr-a-5f8f5c978-xgcpw                     1/1     Running     0          79s\nrook-ceph-mon-a-6879b87656-bxbrw                    1/1     Running     0          89s\nrook-ceph-operator-86f9b59b8-2fvkb                  1/1     Running     0          5m29s\nrook-ceph-osd-0-9dcb46c48-hrzvz                     1/1     Running     0          43s\n</code></pre> NOTE If you want to delete/create a new cluster, besides invoking <code>kubectl delete -f cluster.yaml</code> You need also to delete the paths in defined in <code>dataDirHostPath</code> and directories.path</p> <p>Now we can proceed with installing DLF.</p>"},{"location":"Ceph-Caching/#method-2-testing","title":"Method 2 (Testing)","text":"<p>If you are after maximum performance we strongly advice to set up your ceph cluster according to the method above. However, for testing purposes and/or lacking of disk devices we describe a method to test this inside minikube and provide a script <code>plugins/ceph-cache-plugin/deploy/rook/setup_ceph_cluster.sh</code> that installs rook with csi-lvm storage class. </p>"},{"location":"Ceph-Caching/#minikube-installation","title":"Minikube installation","text":"<p>First we need to have a working cluster.</p> <p><code>minikube start --memory='6G' --cpus=4 --disk-size='40g' --driver=virtualbox -p rooktest</code></p> <p>NOTE: run <code>./minikube/fix_minikube_losetup.py</code> to bypass the current issue of minikube with loset.</p> <p>NOTE2: if you change the disk-size of the minikube command make sure to tune accordingly the following parameters</p>"},{"location":"Ceph-Caching/#csi-lvm-setup","title":"CSI-LVM setup","text":"<p>Before invoking the script you should tune according to your needs the following attributes</p> Attribute File Description GIGA_SPACE <code>plugins/ceph-cache-plugin/deploy/rook/csi-lvm-setup/create-loops.yaml</code> Size of the loop device that csi-lvm will create on each node <code>spec.mon.volumeClaimTemplate.spec.resources.requests.storage</code> <code>plugins/ceph-cache-plugin/deploy/rook/cluster-on-pvc.yaml</code> Storage Size of mon ceph service <code>spec.storage.storageClassDeviceSets.volumeClaimTemplates.spec.resources.requests.storage</code> <code>plugins/ceph-cache-plugin/deploy/rook/cluster-on-pvc.yaml</code> Storage size of CEPH osds <code>spec.storage.storageClassDeviceSets.count</code> <code>plugins/ceph-cache-plugin/deploy/rook/cluster-on-pvc.yaml</code> Total number of CEPH osds <p>The command line arguments of the script are the names of the nodes that the csi-lvm should create loop devices on and the corresponding CEPH services will run on, e.g.</p> <pre><code>cd plugins/ceph-cache-plugin/deploy/rook &amp;&amp; \\\n./setup_ceph_cluster.sh nodename1 ...\n</code></pre> <p>Keep in mind that the script will uninstall any previous installations of csi-lvm and rook-ceph which made through the script. If no command line arguments are passed to the script this will result in uninstalling everything.</p>"},{"location":"Ceph-Caching/#dlf-installation","title":"DLF Installation","text":"<p>Go into the root of this directory and execute: <code>make deployment</code></p> <p>The pods in the default namespace would look like this: <pre><code>csi-attacher-nfsplugin-0            2/2     Running   0          7s\ncsi-attacher-s3-0                   1/1     Running   0          8s\ncsi-nodeplugin-nfsplugin-nqgtl      2/2     Running   0          7s\ncsi-provisioner-s3-0                2/2     Running   0          8s\ncsi-s3-k9b5j                        2/2     Running   0          8s\ndataset-operator-7b8f65f7d4-hg8n5   1/1     Running   0          6s\n</code></pre> Create an s3 dataset by replacing the values and invoking <code>kubectl create -f my-dataset.yaml</code> <pre><code>apiVersion: com.ie.ibm.hpsys/v1alpha1\nkind: Dataset\nmetadata:\nname: example-dataset\nspec:\nlocal:\ntype: \"COS\"\naccessKeyID: \"{AWS_ACCESS_KEY_ID}\"\nsecretAccessKey: \"{AWS_SECRET_ACCESS_KEY}\"\nendpoint: \"{S3_SERVICE_URL}\"\nbucket: \"{BUCKET_NAME}\"\nregion: \"\" #it can be empty\n</code></pre></p> <p>Now if you check about datasetsinternal and PVC you would be able to see the example-dataset <pre><code>kubectl get datasetsinternal\nkubectl get pvc\n</code></pre> Delete the dataset we created before by executing <code>kubectl delete dataset/example-dataset</code> If you execute <code>kubectl describe datasetinternal/example-dataset</code> you would see the credentials and the endpoints you originally specified.</p> <p>Let's try to add the caching plugin.</p>"},{"location":"Ceph-Caching/#ceph-caching-plugin-installation","title":"Ceph Caching Plugin Installation","text":"<p>Change into the directory and invoke: <code>make deployment</code></p> <p>Let's create the same dataset now that the plugin is deployed: <code>kubectl create -f my-dataset.yaml</code></p> <p>You should see a new rgw pod starting up on rook-ceph namespace: <pre><code>rook-ceph-rgw-test-a-77f78b7b69-z5kp9              1/1     Running     0          4m43s\n</code></pre> After a couple of minutes if you list datasetsinternal you will see the example-dataset created.  If you describe it using <code>kubectl describe datasetinternal/example-dataset</code> you will notice that the credentials are different and they point to the rados gateway instance, therefore the PVC would reflect the cached version of the dataset.</p>"},{"location":"FAQ/","title":"FAQ","text":""},{"location":"FAQ/#what-is-the-framework-offering-exactly","title":"What is the framework offering exactly?","text":"<p>One new Custom Resource Definition: the Dataset. Essentially this CRD is a declarative way to reference an existing data source. Moreover, we provide a mount-point in user's pod for each Dataset and expose an interface for caching mechanisms to leverage. Current implementation supports S3- and NFS-based data sources.</p>"},{"location":"FAQ/#thats-it-you-just-add-one-more-crd","title":"That's it? You just add one more CRD?","text":"<p>Not quite. For every Dataset we create one Persistent Volume Claim which users can mount directly to their pods. We have implemented that logic as a regular Kubernetes Operator.</p>"},{"location":"FAQ/#what-is-the-motivation-for-this-work-what-problem-does-it-solve","title":"What is the motivation for this work? What problem does it solve?","text":"<p>Since the introduction of Container Storage Interface, there are more and more storage providers becoming available on Kubernetes environments. However we feel that for the non-experienced Kubernetes users it might be a high barrier for them to install/maintain/configure in order to leverage the available CSI plugins and gain access to the remote data sources on their pods.</p> <p>By introducing a higher level of abstraction (Dataset) and by taking care of all the necessary work around invoking the appropriate CSI plugin, configuring and provisioning the PVC we aim to improve the User Experience of data access in Kubernetes</p>"},{"location":"FAQ/#soyou-want-to-replace-csi","title":"So...you want to replace CSI?","text":"<p>On the contrary! Every type of data source we support actually comes with its own completely standalone CSI implementation.</p> <p>We are aspiring to be a meta-framework for the CSI plugins. If we have to make a comparison, we want make accessible different types of data sources  the same way Kubeflow makes Machine Learning frameworks accessible on Kubernetes</p>"},{"location":"FAQ/#are-you-competing-with-the-cosi-proposal","title":"Are you competing with the COSI proposal?","text":"<p>Absolutely no. COSI aims to manage the full lifecycle of a bucket like provisioning, configuring access etc. which is beyond our scope. We just want to offer a mountpoint for COS buckets</p>"},{"location":"FAQ/#any-other-potential-benefits-you-see-with-the-framework","title":"Any other potential benefits you see with the framework?","text":"<p>We believe that by introducing Dataset as a CRD you can accomplish higher level orchestration and bring contributions on: - Performance: We have attempted to create a pluggable caching interface like the example implementation: Ceph Caching Plugin - Security: Another effort we are exploring is to have a common access management layer for credentials of the different types of datasources </p>"},{"location":"FAQ/#is-anyone-actually-interested-in-the-framework","title":"Is anyone actually interested in the framework?","text":"<ul> <li>European Bioinformatics Institute ( https://www.ebi.ac.uk/ ) are running a POC with Datashim and Kubeflow on their cloud infrastructure</li> <li>David Yu Yuan actually reached out to us after a CNCF presentation</li> <li>People from Open Data Hub ( https://opendatahub.io/ ) are interested in integrating Datashim in ODH</li> <li>See relevant issue ( https://github.com/IBM/dataset-lifecycle-framework/issues/40 )</li> <li>Pachyderm's proposal is actually very close to the Dataset spec we are supporting.</li> <li>Datashim is forked in their repo and is under evaluation in their repo https://github.com/pachyderm/kfdata</li> </ul>"},{"location":"GitWorkflow/","title":"Git workflow for Datashim development","text":"<p>We'll roughly follow the Github development flow used by the Kubernetes project. </p> <ol> <li> <p>Visit https://github.com/datashim-io/datashim. Fork your own copy of Datashim to your Github account. For the sake of illustration, let's say this fork corresponds to <code>https://github.com/$user/datashim</code> where <code>$user</code> is your username.</p> </li> <li> <p>Go to the source directory of your Go workspace and clone your fork there. Using the example above where the workspace is in <code>$HOME/goprojects</code>,    <pre><code>$&gt; mkdir -p $HOME/goprojects/src\n$&gt; cd $HOME/goprojects/src\n$&gt; git clone https://github.com/$user/datashim.git\n</code></pre></p> </li> <li> <p>Set the Datashim repo as your upstream and rebase    <pre><code>$&gt; cd $HOME/goprojects/src/datashim\n$&gt; git remote add upstream https://github.com/datashim-io/datashim\n$&gt; git remote set-url --push upstream no_push \n</code></pre>    The last line prevents pushing to upstream. You can verify your remotes by <code>git remote -v</code> <pre><code>$&gt; git fetch upstream\n$&gt; git checkout master\n$&gt; git rebase upstream/master\n</code></pre></p> </li> <li> <p>Create a new branch to work on a feature or fix. Before this, please create an issue in the main Datashim repository that describes the problem or feature. Note the issue number (e.g. <code>nnn</code>) and assign it to yourself. In your local repository, create a branch to work on the fix. Use a short title (2 or 3 words) formed from the issue title/description along with the issue number as the branch name </p> </li> </ol> <p><pre><code>$&gt; git checkout -b nnn-short-title\n</code></pre>    Make your changes. Then commit your changes. Always sign your commits <pre><code>$&gt; git commit -s -m \"short descriptive message\"\n$&gt; git push $your_remote nnn-short-title\n</code></pre></p> <ol> <li> <p>When you are ready to submit a Pull Request (PR) for your completed feature or branch, visit your fork on Github and click the button titled <code>Compare and Pull Request</code> next to your <code>nnn-short-title</code> branch. This will submit the PR to Datashim.io for review</p> </li> <li> <p>After the review, prepare your PR for merging by squashing your commits. </p> </li> </ol>"},{"location":"GolangVSCodeGit/","title":"Recommended environment setup for development","text":""},{"location":"GolangVSCodeGit/#setting-up-go-and-vscode","title":"Setting up Go and VSCode","text":"<ol> <li> <p>Visit https://go.dev/doc/install to download and install Go on your computer. Alternatively, you can also use package managers for your operating system (e..g Homebrew for macOS)</p> </li> <li> <p>Once installed, run <code>go version</code> to verify that the installation is working</p> </li> <li> <p>(Recommended) Go uses a variable <code>GOPATH</code> to point to the current workspace. Package install commands such as <code>go install</code> will use this as their destination. If you are using a package as well as extending it, then it would be better to set up a separate workspace for development. To do this, create a separate directory, e.g. <code>$HOME/goprojects</code> and set it up with <code>bin</code>,<code>src</code>, and <code>pkg</code> sub-directories, and set <code>GOPATH</code> to point to it when developing. You can also use VSCode to modify <code>GOPATH</code> per project (see below)</p> </li> <li> <p>Download VSCode from https://code.visualstudio.com/download. Open Extensions tab and search for Go or go to https://marketplace.visualstudio.com/items?itemName=golang.go. Verify that the extension is by Go team at Google. Install extension to VSCode and test it with a sample program</p> </li> </ol>"},{"location":"GolangVSCodeGit/#setting-up-datashim-in-vscode","title":"Setting up Datashim in VSCode","text":"<p>Before following the below suggestions, please ensure that you have checked out Datashim following the git workflow for development. Datashim is a collection of multiple Go projects including the Dataset Operator, CSI-S3, Ceph Cache plugin, etc. Therefore, the VSCode setup is not as straightforward as with a single Go project. </p> <ol> <li> <p>Start VSCode. Open a new window (File -&gt; New Window). Select the Explorer view (generally the topmost icon on the left pane)</p> </li> <li> <p>Add a folder to the workspace (File -&gt; Add Folder To Workspace). In the file picker dialog, traverse to <code>$HOME/goprojects/src/github.com/$user/datashim</code> and then deeper into subprojects (i.e. <code>src/</code> folder). At this point, add the subfolder representing the project that you want to work on (e.g. <code>dataset-operator</code>). Do not add the project root folder to the VSCode workspace.</p> </li> <li> <p>Your Explorer view will have the project in the side panel like so:</p> </li> </ol> <p></p> <ol> <li>If you have followed the advice of having a separate directory for go projects, you need to inform Go plugin in VSCode about it. Open Preferences -&gt; Settings. Click on User or Workspace tab. On the left pane, click on Extensions -&gt; Go and scroll down to Gopath on the right-hand pane like so:</li> </ol> <p> </p> <ol> <li>Add these lines to the JSON file:    <pre><code>\"go.toolsGopath\": \"$HOME/go\",\n\"go.gopath\": \"$HOME/goprojects\",\n</code></pre>    where the first line is the Go installation folder and the second line is the folder you've created for hacking.</li> </ol>"},{"location":"Roadmap/","title":"Roadmap","text":"<p>The order of the features/milestones represents loosely the order of which development will start.</p>"},{"location":"Roadmap/#noobaa-caching-plugin","title":"Noobaa Caching Plugin","text":"<p>The S3-to-S3 caching is currently only supported by the Ceph/Rook-based plugin. However, we have been facing various problems as it's setup/configuration is not fully dynamic the way Noobaa is.</p> <p>In the wiki Caching-Remote-Buckets-(User-Guide) we have few hints about how to provision the cache buckets and this logic would be reflected on the Noobaa Caching Plugin</p>"},{"location":"Roadmap/#object-bucket-api","title":"Object Bucket API","text":"<p>Our current approach is based on our modified version of csi-s3 which is not maintained. The Object Bucket API will reduce the code we have to maintain as the S3 operations would be supported in a more K8s native manner with the new API.</p> <p>All the S3-related operations should be replaced with the Object Bucket API once it's ready to be used.</p>"},{"location":"Roadmap/#vault-based-access-management","title":"Vault-based access management","text":"<p>In our current approach, for the datasets which require credentials are stored in secrets. Secrets is the de-facto kubernetes solution for storing credentials. However there are some problems when it comes to datasets. We might want to restrict the access to the datasets between the users in the same namespace. We would be able to support scenarios where UserA and UserB are on the same namespace but UserA has datasets which only they can access.</p> <p>Plan to leverage TSI</p>"},{"location":"Roadmap/#spectrum-scale-caching-plugin","title":"Spectrum Scale Caching Plugin","text":"<p>Assuming Spectrum Scale installed on hosts we could leverage ibm-spectrum-scale-csi to provide the same functionality of S3 caching as Ceph-based and Noobaa-based.</p>"},{"location":"Roadmap/#dataset-eviction-from-cache","title":"Dataset Eviction from cache","text":"<p>In our current approach, in the one implementation we have of a caching plugin, every dataset is being cached without priorities or checks (whether the cache is full etc). We need to tackle this. </p> <p>The most naive way to solve it is to not to use cache for a newly created dataset when the cache is full. A more sophisticated approach would be to monitor the usage of datasets and decide to evict based on some configurable policies.</p>"},{"location":"Roadmap/#sequential-transformation-of-datasets","title":"Sequential Transformation of Datasets","text":"<p>In our current approach, the only possible transformation we have is Dataset -&gt; DatasetInternal -&gt; PVCs. In the future we would like to be able to support any number of transformation of any type. So there would be plugins that can handle a flow like this: Dataset(s3) -(caching)-&gt; DatasetInternal(s3) -(expose)-&gt; DatasetInternal(NFS) -&gt; PVC That would give the users the capability to cache and export their datasets in the format of their preference.</p>"},{"location":"Roadmap/#simple-scheduling-hints","title":"Simple Scheduling Hints","text":"<p>Since we are aware of the nodes where a dataset is cached we can potentially offer this information to external schedulers or decorate the pods using <code>nodeAffinity</code> to assist the default Kubernetes scheduler to place the pods closer to the cached data. This is expected to improve the performance of the pods using the specific datasets.</p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/","title":"Data Volumes for Notebook Servers","text":"<p>We will show how you can use DLF to provision Data Volumes for your notebook servers. This would be helpful in the cases your training data are stored in S3 Buckets.</p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#requirements","title":"Requirements","text":"<p>You have access to the kubeflow dashboard and you have DLF installed.</p> <p>Make sure you first follow the guide for Installation</p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#create-a-dataset-for-the-s3-bucket","title":"Create a Dataset for the S3 Bucket","text":"<p>In this guide, we assume that your data are already stored in a remote s3 bucket. Let's assume that you will launch your notebook server on the namespace <code>{my-namespace}</code></p> <p><pre><code>apiVersion: com.ie.ibm.hpsys/v1alpha1\nkind: Dataset\nmetadata:\nname: your-dataset\nspec:\nlocal:\ntype: \"COS\"\naccessKeyID: \"access_key_id\"\nsecretAccessKey: \"secret_access_key\"\nendpoint: \"https://YOUR_ENDPOINT\"\nbucket: \"YOUR_BUCKET\"\nregion: \"\" #it can be empty\n</code></pre> Now just execute: <pre><code>kubectl create -f my-dataset.yaml -n {my-namespace}\n</code></pre></p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#provision-notebook-with-the-data-volume","title":"Provision Notebook with the Data Volume","text":"<p>Now use the Kubeflow Central Dashboard to follow the rest of the guide. Choose the \"Notebook Servers\" item:</p> <p></p> <p>Select \"New server\":</p> <p></p> <p>Head over to the \"Data Volumes\" section and fill out the form as follows:</p> <p></p> <p>Now you can press \"Launch\" to start the notebook server.</p> <p>After you connect, you can list the contents of <code>/mnt/dataset</code> and verity that the reflect the contents for your remote S3 bucket. NOTE: all the changes that you do in this directory (delete,create,modify) will be reflected on the remote bucket</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/","title":"Model Storing and Serving","text":"<p>We will show how you can use DLF to store/serve trained models using S3 Buckets.</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#requirements","title":"Requirements","text":"<p>You have permissions in a namespace where you can use for kubeflow (to create TFJobs, deployments etc) Lets assume the namespace you can use is <code>{my-namespace}</code>. Feel free to change accordingly.</p> <p>Make sure you first follow the guide for Installation</p> <p>We will loosely follow the example posted in mnist_vanilla_k8s.ipynb</p> <p>NOTE: All example yaml files mentioned in the wiki are also available in examples/kubeflow</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#build-model-container","title":"Build model container","text":"<p>There is a delta between existing distributed mnist examples and what's needed to run well as a TFJob. We will skip the kaniko part and just build and use the Dockerfile and model.py in examples/kubeflow</p> <pre><code>cd examples/kubeflow\ndocker build -t {MY-REGISTRY}/mnist-model -f Dockerfile.model .\ndocker push {MY-REGISTRY}/mnist-model\n</code></pre> <p>In case you use an authenticated registry, follow the instructions in configure-docker-credentials</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#create-an-s3-bucket-and-its-dataset","title":"Create an S3 Bucket and its Dataset","text":"<p>If you have an existing s3 bucket you can use, please proceed with this one. Otherwise follow the instructions in Configure IBM COS Storage</p> <p>Now we need to create a dataset to point to the newly created bucket. Create a file that looks like this: <pre><code>apiVersion: com.ie.ibm.hpsys/v1alpha1\nkind: Dataset\nmetadata:\nname: your-dataset\nspec:\nlocal:\ntype: \"COS\"\naccessKeyID: \"access_key_id\"\nsecretAccessKey: \"secret_access_key\"\nendpoint: \"https://YOUR_ENDPOINT\"\nbucket: \"YOUR_BUCKET\"\nregion: \"\" #it can be empty\n</code></pre> Now just execute: <pre><code>kubectl create -f my-dataset.yaml -n {my-namespace}\n</code></pre></p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#launch-a-tfjob","title":"Launch a TFJob","text":"<p>Now we are ready to launch a tfjob in a much less verbose way since DLF takes care of mounting the dataset and providing access to the tensorflow pod: <pre><code>apiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\nname: my-train\nspec:\ntfReplicaSpecs:\nPs:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\ndataset.0.id: \"your-dataset\"\ndataset.0.useas: \"mount\"\nannotations:\nsidecar.istio.io/inject: \"false\"\nspec:\nserviceAccount: default-editor\ncontainers:\n- name: tensorflow\ncommand:\n- python\n- /opt/model.py\n- --tf-model-dir=/mnt/datasets/your-dataset/mnist\n- --tf-export-dir=/mnt/datasets/your-dataset/mnist/export\n- --tf-train-steps=200\n- --tf-batch-size=100\n- --tf-learning-rate=0.1\nimage: yiannisgkoufas/mnist\nworkingDir: /opt\nresources:\nlimits:\nephemeral-storage: \"10Gi\"\nrequests:\nephemeral-storage: \"10Gi\"\nrestartPolicy: OnFailure\nChief:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\ndataset.0.id: \"your-dataset\"\ndataset.0.useas: \"mount\"\nannotations:\nsidecar.istio.io/inject: \"false\"\nspec:\nserviceAccount: default-editor\ncontainers:\n- name: tensorflow\nresources:\nlimits:\nephemeral-storage: \"10Gi\"\nrequests:\nephemeral-storage: \"10Gi\"\ncommand:\n- python\n- /opt/model.py\n- --tf-model-dir=/mnt/datasets/your-dataset/mnist\n- --tf-export-dir=/mnt/datasets/your-dataset/mnist/export\n- --tf-train-steps=200\n- --tf-batch-size=100\n- --tf-learning-rate=0.1\nimage: yiannisgkoufas/mnist\nrestartPolicy: OnFailure\nWorker:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\ndataset.0.id: \"your-dataset\"\ndataset.0.useas: \"mount\"\nannotations:\nsidecar.istio.io/inject: \"false\"\nspec:\nserviceAccount: default-editor\ncontainers:\n- name: tensorflow\ncommand:\n- python\n- /opt/model.py\n- --tf-model-dir=/mnt/datasets/your-dataset/mnist\n- --tf-export-dir=/mnt/datasets/your-dataset/mnist/export\n- --tf-train-steps=200\n- --tf-batch-size=100\n- --tf-learning-rate=0.1\nimage: yiannisgkoufas/mnist\nworkingDir: /opt\nrestartPolicy: OnFailure\n</code></pre> Make sure to replace <code>your-dataset</code> with the name of your dataset. Create the TFJob like that: <pre><code>kubectl create -f tfjob.yaml -n {my-namespace}\n</code></pre> You should see the job running and the model stored in the end in the remote S3 bucket.</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#view-the-model-in-tensorboard","title":"View the Model in Tensorboard","text":"<p>You can inspect the model you created and stored in the remote S3 bucket by creating the following yaml file which again leverages the Dataset created. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: mnist-tensorboard\nname: mnist-tensorboard\nspec:\nselector:\nmatchLabels:\napp: mnist-tensorboard\ntemplate:\nmetadata:\nlabels:\napp: mnist-tensorboard\nversion: v1\ndataset.0.id: \"your-dataset\"\ndataset.0.useas: \"mount\"\nannotations:\nsidecar.istio.io/inject: \"false\"\nspec:\nserviceAccount: default-editor\ncontainers:\n- command:\n- /usr/local/bin/tensorboard\n- --logdir=/mnt/datasets/your-dataset/mnist\n- --port=80\nimage: tensorflow/tensorflow:1.15.2-py3\nname: tensorboard\nports:\n- containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp: mnist-tensorboard\nname: mnist-tensorboard\nspec:\nports:\n- name: http-tb\nport: 80\ntargetPort: 80\nselector:\napp: mnist-tensorboard\ntype: ClusterIP\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: mnist-tensorboard\nspec:\ngateways:\n- kubeflow/kubeflow-gateway\nhosts:\n- '*'\nhttp:\n- match:\n- uri:\nprefix: /mnist/default/tensorboard/\nrewrite:\nuri: /\nroute:\n- destination:\nhost: mnist-tensorboard.default.svc.cluster.local\nport:\nnumber: 80\ntimeout: 300s\n</code></pre> Create the deployment: <pre><code>kubectl create -f tensorboard.yaml -n {my-namespace}\n</code></pre> You can expose the service and access it remotely as described here: Tensorboard access</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#model-serving-using-kfserving","title":"Model Serving Using KFServing","text":"<p>You can leverage DLF to run the inference service on the model you trained using KFServing as follows: <pre><code>apiVersion: \"serving.kubeflow.org/v1alpha2\"\nkind: \"InferenceService\"\nmetadata:\nname: \"mnist-sample\"\nspec:\ndefault:\npredictor:\ntensorflow:\nstorageUri: \"pvc://your-dataset/mnist/export\"\n</code></pre> Create the yaml: <pre><code>kubectl create -f kfserving-inference.yaml -n {my-namespace}\n</code></pre></p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#model-serving-using-tensorflow-serving","title":"Model Serving Using Tensorflow Serving","text":"<p>Again you can leverage DLF to serve the model you trained.  <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: mnist\nname: tensorflow-serving\nspec:\nselector:\nmatchLabels:\napp: mnist-model\ntemplate:\nmetadata:\nannotations:\nsidecar.istio.io/inject: \"false\"\nlabels:\napp: mnist-model\nversion: v1\ndataset.0.id: \"your-dataset\"\ndataset.0.useas: \"mount\"\nspec:\nserviceAccount: default-editor\ncontainers:\n- args:\n- --port=9000\n- --rest_api_port=8500\n- --model_name=mnist\n- --model_base_path=/mnt/datasets/your-dataset/mnist/export\ncommand:\n- /usr/bin/tensorflow_model_server\nenv:\n- name: modelBasePath\nvalue: /mnt/datasets/your-dataset/mnist/export\nimage: tensorflow/serving:1.15.0\nimagePullPolicy: IfNotPresent\nlivenessProbe:\ninitialDelaySeconds: 30\nperiodSeconds: 30\ntcpSocket:\nport: 9000\nname: mnist\nports:\n- containerPort: 9000\n- containerPort: 8500\nresources:\nlimits:\ncpu: \"4\"\nmemory: 4Gi\nrequests:\ncpu: \"1\"\nmemory: 1Gi\nvolumeMounts:\n- mountPath: /var/config/\nname: model-config\nvolumes:\n- configMap:\nname: tensorflow-serving\nname: model-config\n---\napiVersion: v1\nkind: Service\nmetadata:\nannotations:\nprometheus.io/path: /monitoring/prometheus/metrics\nprometheus.io/port: \"8500\"\nprometheus.io/scrape: \"true\"\nlabels:\napp: mnist-model\nname: tensorflow-serving\nspec:\nports:\n- name: grpc-tf-serving\nport: 9000\ntargetPort: 9000\n- name: http-tf-serving\nport: 8500\ntargetPort: 8500\nselector:\napp: mnist-model\ntype: ClusterIP\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\nname: tensorflow-serving\ndata:\nmonitoring_config.txt: |-\nprometheus_config: {{\nenable: true,\npath: \"/monitoring/prometheus/metrics\"\n}}\n</code></pre> Now create the deployment: <pre><code>kubectl create -f tensorflow-serving -n {my-namespace}\n</code></pre></p> <p>If you want to deploy the demo with the MNIST UI follow the instructions in MNIST UI</p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/","title":"PVCs for Pipelines SDK","text":"<p>We will show how you can use DLF to provision Persistent Volume Claims via DLF so you can use it within Pipelines SDK.</p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#requirements","title":"Requirements","text":"<p>You have kubeflow installed and you can deploy pipelines using the Pipeline SDK.</p> <p>Make sure you first follow the guide for Installation</p> <p>We will just how you can adopt the examples located in contrib/volume_ops</p> <p>NOTE: For this guide you can use both an empty and pre-populated with data bucket.</p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#example-with-creation-of-dataset-before-the-pipeline-execution","title":"Example with creation of Dataset before the pipeline execution","text":"<p>First you need to create a Dataset to point to the bucket you want to use. Create a file that looks like this: <pre><code>apiVersion: com.ie.ibm.hpsys/v1alpha1\nkind: Dataset\nmetadata:\nname: your-dataset\nspec:\nlocal:\ntype: \"COS\"\naccessKeyID: \"access_key_id\"\nsecretAccessKey: \"secret_access_key\"\nendpoint: \"https://YOUR_ENDPOINT\"\nbucket: \"YOUR_BUCKET\"\nregion: \"\" #it can be empty\n</code></pre> Now just execute: <pre><code>kubectl create -f my-dataset.yaml -n {my-namespace}\n</code></pre></p> <p>Now within <code>{my-namespace}</code> you will find a PVC which you can use within your pipelines SDK without a problem.</p> <p>You can see the example below which can use the PVC which was created out of your dataset. <pre><code>import kfp\nimport kfp.dsl as dsl\nfrom kfp.dsl import PipelineVolume\n\n\n@dsl.pipeline(\n    name=\"Volume Op DAG\",\n    description=\"The second example of the design doc.\"\n)\ndef volume_op_dag():\n\n    dataset = PipelineVolume(\"your-dataset\")\n\n    step1 = dsl.ContainerOp(\n        name=\"step1\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo 1|tee /data/file1\"],\n        pvolumes={\"/data\": dataset}\n    )\n\n    step2 = dsl.ContainerOp(\n        name=\"step2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"cp /data/file1 /data/file2\"],\n        pvolumes={\"/data\": step1.pvolume}\n    )\n\n    step3 = dsl.ContainerOp(\n        name=\"step3\",\n        image=\"library/bash:4.4.23\",\n        command=[\"cat\", \"/mnt/file1\", \"/mnt/file2\"],\n        pvolumes={\"/mnt\": step2.pvolume}\n    )\n\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(volume_op_dag, __file__ + \".tar.gz\")\n</code></pre></p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#example-with-creation-of-dataset-as-part-of-the-pipeline-execution","title":"Example with creation of Dataset as part of the pipeline execution","text":"<p>If instead you want to create a Dataset as part of your pipeline, you can create the Dataset yaml and invoke a <code>ResourceOp</code>.</p> <p>Before that you need to make sure that the service account <code>pipeline-runner</code> in namespace <code>kubeflow</code> can create/delete Datasets, so make sure you execute <code>kubectl apply -f examples/kubeflow/pipeline-runner-binding.yaml</code> before running the pipeline. The example rolebinding definition is in examples/kubeflow/pipeline-runner-binding.yaml</p> <p>In the following pipeline we are creating the Dataset in step0 and then proceed to step1 to use it:</p> <pre><code>import kfp.dsl as dsl\nimport yaml\nfrom kfp.dsl import PipelineVolume\n\n# Make sure that you have applied ./pipeline-runner-binding.yaml\n# or any serviceAccount that should be allowed to create/delete datasets\n\n@dsl.pipeline(\n    name=\"Volume Op DAG\",\n    description=\"The second example of the design doc.\"\n)\ndef volume_op_dag():\n\n    datasetName = \"your-dataset\"\n    dataset = PipelineVolume(datasetName)\n\n    step0 = dsl.ResourceOp(name=\"dataset-creation\",k8s_resource=get_dataset_yaml(\n        datasetName,\n        \"XXXXXXXXXXXXXXX\",\n        \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n        \"http://your_endpoint.com\",\n        \"bucket-name\",\n        \"\"\n    ))\n\n    step1 = dsl.ContainerOp(\n        name=\"step1\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo 1|tee /data/file1\"],\n        pvolumes={\"/data\": dataset}\n    ).after(step0)\n\n    step2 = dsl.ContainerOp(\n        name=\"step2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"cp /data/file1 /data/file2\"],\n        pvolumes={\"/data\": step1.pvolume}\n    )\n\n    step3 = dsl.ContainerOp(\n        name=\"step3\",\n        image=\"library/bash:4.4.23\",\n        command=[\"cat\", \"/mnt/file1\", \"/mnt/file2\"],\n        pvolumes={\"/mnt\": step2.pvolume}\n    )\n\ndef get_dataset_yaml(name,accessKey,secretAccessKey,endpoint,bucket,region):\n    print(region)\n    dataset_spec = f\"\"\"\n    apiVersion: com.ie.ibm.hpsys/v1alpha1\n    kind: Dataset\n    metadata:\n      name: {name}\n    spec:\n      local:\n        type: \"COS\"\n        accessKeyID: {accessKey}\n        secretAccessKey: {secretAccessKey}\n        endpoint: {endpoint}\n        bucket: {bucket}\n        region: {region}\n    \"\"\"\n    data = yaml.safe_load(dataset_spec)\n    convert_none_to_str(data)\n    return data\n</code></pre>"}]}